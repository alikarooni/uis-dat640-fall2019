{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2: Document-at-a-time scoring\n",
    "\n",
    "Implement term-at-a-time scoring using vector space retrieval with TFIDF term weighting and dot product similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td_matrix = {\n",
    "    \"beijing\": [0, 1, 0, 0, 1],\n",
    "    \"dish\": [0, 1, 0, 0, 1],\n",
    "    \"duck\": [3, 2, 2, 0, 1],\n",
    "    \"rabbit\": [0, 0, 1, 1, 0],\n",
    "    \"recipe\": [0, 0, 1, 1, 1],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of documents is set manually for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_DOCS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the corresponding inverted index\n",
    "\n",
    "The postings hold (docID, freq) pairs. docID indices start from 0\n",
    "\n",
    "`doclen` stores the document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beijing': [(1, 1), (4, 1)],\n",
      " 'dish': [(1, 1), (4, 1)],\n",
      " 'duck': [(0, 3), (1, 2), (2, 2), (4, 1)],\n",
      " 'rabbit': [(2, 1), (3, 1)],\n",
      " 'recipe': [(2, 1), (3, 1), (4, 1)]}\n"
     ]
    }
   ],
   "source": [
    "inv_idx = {}\n",
    "doclen = {}\n",
    "for term, vec in td_matrix.items():\n",
    "    inv_idx[term] = []\n",
    "    for doc_id, freq in enumerate(vec):\n",
    "        if freq > 0:\n",
    "            inv_idx[term].append((doc_id, freq))\n",
    "            doclen[doc_id] = doclen.get(doc_id, 0) + freq\n",
    "\n",
    "pprint(inv_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This class provides access to the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InvIndex(object):\n",
    "    def __init__(self, idx_contents):\n",
    "        self.idx = idx_contents\n",
    "    \n",
    "    def postings(self, term):\n",
    "        return self.idx.get(term, [])\n",
    "    \n",
    "    def get_posting_pos(self, term, pos):\n",
    "        if term not in self.idx:\n",
    "            return (None, 0)\n",
    "        if len(self.idx[term]) <= pos:\n",
    "            return (None, 0)\n",
    "        return self.idx[term][pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the InvIndex class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idx = InvIndex(inv_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def idf(term):\n",
    "    return math.log(NUM_DOCS / len(idx.postings(term)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-at-a-time scoring\n",
    "\n",
    "We utilize the fact that the posting lists are ordered by document ID. \n",
    "The posting lists of the query terms are iterated parallel to each other (we always read from the beginning of the list and delete the posting once the current document has been processed).\n",
    "Each document is scored according to\n",
    "\n",
    "$score(q,d) = \\sum_{t \\in q} w_{t,d} \\times w_{t,q}$\n",
    "\n",
    "where $w_{t,d}=tf_{t,d}\\times idf_t$ and $w_{t,q}=tf_{t,q}$\n",
    "\n",
    "  - Use normalized frequencies for TF weight, i.e., $tf_{t,d}=\\frac{f_{t,d}}{|d|}$, where $f_{t,d}$ is the number of occurrences of term $t$ in document $d$ and $|d|$ is the document length (=total number of terms). (It goes analogously for the query.)\n",
    "  - Compute IDF values using the following formula: $idf_{t}=\\log \\frac{N}{n_t}$, where $N$ is the total number of document and $n_t$ is the number of documents that contain term $t$. \n",
    "\n",
    "(I.e., the same as in Exercise #1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score_dt(query, index):\n",
    "    # Change the sequence of query terms into a \"term: freq\" dictionary\n",
    "    qry = dict(Counter(query))\n",
    "\n",
    "    scores = {}  # Holds the final document scores (this should be a priority list, but for simplicity we use a dictionary here)\n",
    "    \n",
    "    pos = {term: 0 for term in qry}  # Holds a pointer for each query term's posting list\n",
    "    \n",
    "    # Iterate through each document\n",
    "    for doc_id in range(NUM_DOCS):            \n",
    "        # First, we collect the document term frequencies from the index\n",
    "        # (Essentially, we just \"recover\" the document's contents from the index.)\n",
    "        f_td = {}  # Holds the term frequencies in the document\n",
    "        for term in qry.keys(): \n",
    "            # Get the frequency of query term i from the posting list\n",
    "            # Utilize the fact that the posting lists are ordered by document ID!\n",
    "            (d, freq) = idx.get_posting_pos(term, pos[term])\n",
    "            if d == doc_id:\n",
    "                f_td[term] = freq\n",
    "                pos[term] += 1\n",
    "            else\n",
    "                # This means that d > doc_id, i.e., the query term is not present in this doc\n",
    "                pass\n",
    "                    \n",
    "        # Then, we score the document\n",
    "        score = 0  # Holds the document's retrieval score\n",
    "        for term, f_tq in qry.items():\n",
    "            # Incement the document's score according to the given query term\n",
    "            tfidf_td = f_td.get(term, 0) / doclen[doc_id] * idf(term)\n",
    "            tf_tq = f_tq / len(query)\n",
    "            score += tfidf_td * tf_tq\n",
    "        # Final document score\n",
    "        scores[doc_id] = score\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = [\"beijing\", \"duck\", \"recipe\"]\n",
    "scores = score_dt(query, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D5: 0.138\n",
      "D2: 0.114\n",
      "D4: 0.085\n",
      "D3: 0.08\n",
      "D1: 0.074\n"
     ]
    }
   ],
   "source": [
    "for doc_id, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(\"D\" + str(doc_id + 1) + \":\", round(score, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/22o3ursi5YsR1Ztb8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
