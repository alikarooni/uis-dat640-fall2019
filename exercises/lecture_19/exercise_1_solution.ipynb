{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1: Getting ordered and unordered bigram matches in Elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing a toy collection \n",
    "\n",
    "This time, we store **term position information** and perform minimal stemming, i.e., removing only plurals (for that, we specify a custom analyzer).\n",
    "\n",
    "Check the [Elasticsearch documentation on analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/current/analyzer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"toy_index\"  \n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    'settings' : {\n",
    "        'index' : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 1\n",
    "        },\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'my_english_analyzer': {\n",
    "                    'type': \"custom\",\n",
    "                    'tokenizer': \"standard\",\n",
    "                    'stopwords': \"_english_\",\n",
    "                    'filter': [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"filter_english_minimal\"\n",
    "                    ]                \n",
    "                }\n",
    "            },\n",
    "            'filter' : {\n",
    "                'filter_english_minimal' : {\n",
    "                    'type': \"stemmer\",\n",
    "                    'name': \"minimal_english\"\n",
    "                },\n",
    "                'english_stop': {\n",
    "                    'type': \"stop\",\n",
    "                    'stopwords': \"_english_\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'title': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'content': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DOCS = {\n",
    "    1: {\"title\": \"Rap God\",\n",
    "        \"content\": \"gonna, gonna, Look, I was gonna go easy on you and not to hurt your feelings\"\n",
    "        },\n",
    "    2: {\"title\": \"Lose Yourself\",\n",
    "        \"content\": \"Yo, if you could just, for one minute Or one split second in time, forget everything Everything that bothers you, or your problems Everything, and follow me\"\n",
    "        },\n",
    "    3: {\"title\": \"Love The Way You Lie\",\n",
    "        \"content\": \"Just gonna stand there and watch me burn But that's alright, because I like the way it hurts\"\n",
    "        },\n",
    "    4: {\"title\": \"The Monster\",\n",
    "        \"content\": [\"gonna gonna I'm friends with the monster\", \"That's under my bed Get along with the voices inside of my head\"]\n",
    "        },\n",
    "    5: {\"title\": \"Beautiful\",\n",
    "        \"content\": \"Lately I've been hard to reach I've been too long on my own Everybody has a private world Where they can be alone\"\n",
    "        },\n",
    "    6: {\"title\": \"Fake Eminem 1\",\n",
    "        \"content\": \"This is not real Eminem, just some text to get more matches for a split second for a split second.\"\n",
    "        },\n",
    "    7: {\"title\": \"Fake Eminem 2\",\n",
    "        \"content\": \"I have a monster friend and I'm friends with the monster and then there are some more friends who are monsters.\"\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'index': 'toy_index', 'shards_acknowledged': True}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "    \n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing our analyzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [{'end_offset': 8,\n",
       "   'position': 0,\n",
       "   'start_offset': 0,\n",
       "   'token': 'monster',\n",
       "   'type': '<ALPHANUM>'},\n",
       "  {'end_offset': 14,\n",
       "   'position': 2,\n",
       "   'start_offset': 12,\n",
       "   'token': 'my',\n",
       "   'type': '<ALPHANUM>'},\n",
       "  {'end_offset': 18,\n",
       "   'position': 3,\n",
       "   'start_offset': 15,\n",
       "   'token': 'bed',\n",
       "   'type': '<ALPHANUM>'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.analyze(index=INDEX_NAME, body={'analyzer': \"my_english_analyzer\", 'text': \"monsters in my bed\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for doc_id, doc in DOCS.items():\n",
    "    es.index(index=INDEX_NAME, id=doc_id, body=doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that you also get term position information when requesting a term vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '2',\n",
      " '_index': 'toy_index',\n",
      " '_type': '_doc',\n",
      " '_version': 1,\n",
      " 'found': True,\n",
      " 'term_vectors': {'content': {'field_statistics': {'doc_count': 7,\n",
      "                                                   'sum_doc_freq': 85,\n",
      "                                                   'sum_ttf': 101},\n",
      "                              'terms': {'bother': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 18}]},\n",
      "                                        'could': {'term_freq': 1,\n",
      "                                                  'tokens': [{'position': 3}]},\n",
      "                                        'everything': {'term_freq': 3,\n",
      "                                                       'tokens': [{'position': 15},\n",
      "                                                                  {'position': 16},\n",
      "                                                                  {'position': 23}]},\n",
      "                                        'follow': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 25}]},\n",
      "                                        'forget': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 14}]},\n",
      "                                        'just': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 4}]},\n",
      "                                        'me': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 26}]},\n",
      "                                        'minute': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 7}]},\n",
      "                                        'one': {'term_freq': 2,\n",
      "                                                'tokens': [{'position': 6},\n",
      "                                                           {'position': 9}]},\n",
      "                                        'problem': {'term_freq': 1,\n",
      "                                                    'tokens': [{'position': 22}]},\n",
      "                                        'second': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 11}]},\n",
      "                                        'split': {'term_freq': 1,\n",
      "                                                  'tokens': [{'position': 10}]},\n",
      "                                        'time': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 13}]},\n",
      "                                        'yo': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 0}]},\n",
      "                                        'you': {'term_freq': 2,\n",
      "                                                'tokens': [{'position': 2},\n",
      "                                                           {'position': 19}]},\n",
      "                                        'your': {'term_freq': 1,\n",
      "                                                 'tokens': [{'position': 21}]}}},\n",
      "                  'title': {'field_statistics': {'doc_count': 7,\n",
      "                                                 'sum_doc_freq': 16,\n",
      "                                                 'sum_ttf': 16},\n",
      "                            'terms': {'lose': {'term_freq': 1,\n",
      "                                               'tokens': [{'position': 0}]},\n",
      "                                      'yourself': {'term_freq': 1,\n",
      "                                                   'tokens': [{'position': 1}]}}}},\n",
      " 'took': 13}\n"
     ]
    }
   ],
   "source": [
    "tv = es.termvectors(index=INDEX_NAME, id=2, fields=\"title,content\")\n",
    "pprint(tv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method returns the sequence of terms for a given document field, with None values for stopwords that got removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_term_sequence(es, doc_id, field):\n",
    "    tv = es.termvectors(index=INDEX_NAME, id=doc_id, fields=[field])\n",
    "    # We first put terms in a position-indexed dict.\n",
    "    pos = {}\n",
    "    for term, tinfo in tv['term_vectors'][field]['terms'].items():\n",
    "        for token in tinfo['tokens']:\n",
    "            pos[token['position']] = term\n",
    "    # Then, turn that dict to a list.\n",
    "    seq = [None] * (max(pos.keys()) + 1)\n",
    "    for p, term in pos.items():\n",
    "        seq[p] = term\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', None, 'monster', 'friend', None, \"i'm\", 'friend', None, None, 'monster', None, None, None, None, 'some', 'more', 'friend', 'who', None, 'monster']\n"
     ]
    }
   ],
   "source": [
    "print(get_term_sequence(es, 7, \"content\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ordered bigram matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of documents that contain the terms \"split second\" in this exact order in the `concent` field.\n",
    "\n",
    "You can use a [match_phrase query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase.html) for example (see the [Elasticsearch notebook](../../code/elasticsearch/Elasticsearch.ipynb) for usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"split second\"\n",
    "bigram = query.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = es.search(index=INDEX_NAME, body={'query': {'match_phrase': {'content': query}}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of those matching documents, count the actual number of times the phrase appears in the `content` field. Note that there is no built-in support in the Elasticsearch API for that; you'll need to get the actual term sequence from the field (using the `get_term_sequence()` helper method) and count it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_ordered_bigram_matches(text, bigram):\n",
    "    \"\"\"Counts the number of bigram matches in text. Both text and bigram are represented as a list of terms.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == bigram[0]:\n",
    "            if text[i + 1] == bigram[1]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#6) Fake Eminem 1 -- 2\n",
      "(#2) Lose Yourself -- 1\n"
     ]
    }
   ],
   "source": [
    "for hit in res['hits']['hits']:\n",
    "    doc_id = hit['_id']\n",
    "    text = get_term_sequence(es, doc_id, \"content\")\n",
    "    count = count_ordered_bigram_matches(text, bigram)\n",
    "    print(\"(#{}) {} -- {}\".format(hit['_id'], hit['_source']['title'], count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting unordered bigram matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a list of documents that contain the terms \"friends monster\" in an unordered window of size 4.\n",
    "\n",
    "You may use Elasticsearch's [span near query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-span-near-query.html) for that. NOTE: for span queries, you'll need to analyze the query terms beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = {\n",
    "    'span_near': {\n",
    "        'clauses': [\n",
    "            {'span_term': {'content': \"friend\"}}, \n",
    "            {'span_term': {'content': \"monster\"}}\n",
    "        ],\n",
    "        'slop': 2,\n",
    "        'in_order': False    \n",
    "    }\n",
    "}\n",
    "bigram=[\"friend\", \"monster\"]\n",
    "\n",
    "res = es.search(index=INDEX_NAME, body={'query': query})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the matching documents, count the actual number of unordered matches in the `content` field. As before, use the `get_term_sequence()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_unordered_bigram_matches(text, bigram, w):\n",
    "    \"\"\"Counts the number of unordered bigram matches in text within a given window size. \n",
    "    Both text and bigram are represented as a list of terms.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in bigram:\n",
    "            other_term = bigram[0] if text[i] == bigram[1] else bigram[1]\n",
    "            if other_term in text[i+1:i+w]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(#7) Fake Eminem 2 -- 3\n",
      "(#4) The Monster -- 1\n"
     ]
    }
   ],
   "source": [
    "for hit in res['hits']['hits']:\n",
    "    doc_id = hit['_id']\n",
    "    text = get_term_sequence(es, doc_id, \"content\")\n",
    "    count = count_unordered_bigram_matches(text, bigram, 4)\n",
    "    print(\"(#{}) {} -- {}\".format(hit['_id'], hit['_source']['title'], count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
