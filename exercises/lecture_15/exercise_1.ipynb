{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1: PRMS for entity ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a local Elasticsearch index of a set of selected movies\n",
    "\n",
    "Build a fielded Elasticsearch index. Fields should include title, description, categories, directors, actors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "INDEX_NAME = \"movies\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM for ranking movies\n",
    "\n",
    "Implement the mixture of language models for ranking movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO update field names and weights\n",
    "FIELDS = [\"title\", \"content\"]\n",
    "FIELD_WEIGHTS = [0.2, 0.8]\n",
    "LAMBDA = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documents should be scored according to **query (log)likelihood**: \n",
    "\n",
    "$\\log P(q|d) = \\sum_{t \\in q} f_{t,q} \\log P(t|\\theta_d)$, \n",
    "\n",
    "where\n",
    "  - $f_{t,q}$ is the frequency of term $t$ in the query\n",
    "  - $P(t|\\theta_d)$ is the (smoothed) document language model.\n",
    "  \n",
    "Using multiple document fields, the **document language model** is taken to be a linear combination of the (smoothed) field language models:\n",
    "\n",
    "$P(t|\\theta_d) = \\sum_i w_i P(t|\\theta_{d_i})$ ,\n",
    "\n",
    "where $w_i$ is the field weight for field $i$ (and $\\sum_i w_i = 1$).\n",
    "\n",
    "The **field language models** $P(t|\\theta_{d_i})$ are computed as follows.\n",
    "\n",
    "Using **Jelinek-Mercer smoothing**:\n",
    "\n",
    "$P(t|\\theta_{d_i}) = (1-\\lambda_i) P(t|d_i) + \\lambda_i P(t|C_i)$,\n",
    "\n",
    "where \n",
    "\n",
    "  - $\\lambda_i$ is a field-specific smoothing parameter\n",
    "  - $P(t|d_i) = \\frac{f_{t,d_i}}{|d_i|}$ is the empirical field language model (term's relative frequency in the document field). $f_{t,d_i}$ is the raw frequency of $t$ in field $i$ of $d$. $|d_i|$ is the length (number of terms) in field $i$ of $d$.\n",
    "  - $P(t|C_i) = \\frac{\\sum_{d'}f_{t,d'_i}}{\\sum_{d'}|d'_i|}$ is the collecting field language model (term's relative frequency in that field across the entire collection)\n",
    "  \n",
    "Using **Dirichlet smoothing**:\n",
    "\n",
    "$p(t|\\theta_{d_i}) = \\frac{f_{t,d_i} + \\mu_i P(t|C_i)}{|d_i| + \\mu_i}$\n",
    "\n",
    "where $\\mu_i$ is the field-specific smoothing parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collection Language Model class\n",
    "\n",
    "This class is used for obtaining collection language modeling probabilities  P(t|Ci)P(t|Ci) .\n",
    "\n",
    "The reason this class is needed is that es.termvectors does not return term statistics for terms that do not appear in the given document. This would cause problems in scoring documents that are partial matches (do not contain all query terms in all fields).\n",
    "\n",
    "The idea is that for each query term, we need to find a document that contains that term. Then the collection term statistics are available from that document's term vector. To make sure we find a matching document, we issue a boolean (match) query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CollectionLM(object):\n",
    "    def __init__(self, es, qterms):\n",
    "        self._es = es\n",
    "        self._probs = {}\n",
    "        # computing P(t|C_i) for each field and for each query term\n",
    "        for field in FIELDS:\n",
    "            self._probs[field] = {}\n",
    "            for t in qterms:\n",
    "                self._probs[field][t] = self._get_prob(field, t)\n",
    "        \n",
    "    def _get_prob(self, field, term):\n",
    "        # Use a boolean query to find a document that contains the term\n",
    "        hits = self._es.search(index=INDEX_NAME, body={\"query\": {\"match\": {field: term}}},\n",
    "                               _source=False, size=1).get(\"hits\", {}).get(\"hits\", {})\n",
    "        doc_id = hits[0][\"_id\"] if len(hits) > 0 else None\n",
    "        if doc_id is not None:\n",
    "            # Ask for global term statistics when requesting the term vector of that doc (`term_statistics=True`)\n",
    "            # TODO: complete this part            \n",
    "            return 0\n",
    "\n",
    "        return 0  # this only happens if none of the documents contain that term\n",
    "\n",
    "    def prob(self, field, term):\n",
    "        return self._probs.get(field, {}).get(term, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_mlm(es, clm, qterms, doc_id):\n",
    "    score = 0  # log P(q|d)\n",
    "    \n",
    "    # Getting term frequency statistics for the given document field from Elasticsearch\n",
    "    # Note that global term statistics are not needed (`term_statistics=False`)\n",
    "    tv = es.termvectors(index=INDEX_NAME, id=doc_id, fields=FIELDS,\n",
    "                              term_statistics=False).get(\"term_vectors\", {})\n",
    "\n",
    "    # compute field lengths $|d_i|$\n",
    "    len_d_i = []  # document field length\n",
    "    for i, field in enumerate(FIELDS):\n",
    "        if field in tv: \n",
    "            len_d_i.append(sum([s[\"term_freq\"] for t, s in tv[field][\"terms\"].items()]))\n",
    "        else:  # that document field may be empty\n",
    "            len_d_i.append(0)\n",
    "        \n",
    "    # scoring the query\n",
    "    for t in qterms:\n",
    "        Pt_theta_d = 0  # P(t|\\theta_d)\n",
    "        for i, field in enumerate(FIELDS):\n",
    "            if field in tv:\n",
    "                Pt_di = tv[field][\"terms\"].get(t, {}).get(\"term_freq\", 0) / len_d_i[i]  # $P(t|d_i)$\n",
    "            else:  # that document field is empty\n",
    "                Pt_di = 0\n",
    "            Pt_Ci = clm.prob(field, t)  # $P(t|C_i)$\n",
    "            Pt_theta_di = (1 - LAMBDA) * Pt_di + LAMBDA * Pt_Ci  # $P(t|\\theta_{d_i})$ with J-M smoothing\n",
    "            Pt_theta_d += FIELD_WEIGHTS[i] * Pt_theta_di\n",
    "        score += math.log(Pt_theta_d)    \n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring queries\n",
    "\n",
    "Perform an initial retrieval using the default ranking in Elasticsearch, then re-score each document using `score_mlm()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"TODO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Elasticsearch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-83942d487e94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElasticsearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Elasticsearch' is not defined"
     ]
    }
   ],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'es' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2b90d5d36347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get top 100 docs using BM25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mINDEX_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"content\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hits'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# re-score docs using MLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'es' is not defined"
     ]
    }
   ],
   "source": [
    "# get top 100 docs using BM25\n",
    "res = es.search(index=INDEX_NAME, q=query, df=\"content\", _source=False, size=200).get('hits', {})\n",
    "\n",
    "# re-score docs using MLM\n",
    "\n",
    "# TODO: get analyzed query\n",
    "qterms = []\n",
    "\n",
    "# get collection LM \n",
    "# (this needs to be instantiated only once per query and can be used for scoring all documents)\n",
    "clm = CollectionLM(es, qterms)        \n",
    "scores = {}\n",
    "for doc in res.get(\"hits\", {}):\n",
    "    doc_id = doc.get(\"_id\")\n",
    "    scores[doc_id] = score_mlm(es, clm, qterms, doc_id)\n",
    "\n",
    "# TODO output top 5 documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRMS\n",
    "\n",
    "Implement field-specific term weighting using PRMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
