{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1: Building an inverted index\n",
    "\n",
    "  - You are given a sample (1000 documents) from the [The Reuters-21578 data collection](http://www.daviddlewis.com/resources/testcollections/reuters21578/) in `data/reuters21578-000.xml`\n",
    "  - The code that parses the XML and extract a list of preprocessed terms (tokenized, lowercased, stopwords removed) is already given\n",
    "  - You are also given an `InvertedIndex` class that manages the posting lists operations\n",
    "  - Your tasks will include:\n",
    "    1. Creating a unit test for the indexing class\n",
    "    2. Building an inverted index from the input collection\n",
    "    3. Using NLTK for text preprocessing\n",
    "    4. Including bigrams as well in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOPWORDS = [\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripping tags inside <> using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def striptags(text):\n",
    "    p = re.compile(r'<.*?>')\n",
    "    return p.sub('', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse input text and return a list of indexable terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(text):\n",
    "    terms = []\n",
    "    # Replace specific characters with space\n",
    "    chars = [\"'\", \".\", \":\", \",\", \"!\", \"?\", \"(\", \")\"]\n",
    "    for ch in chars:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, \" \")\n",
    "\n",
    "    # Remove tags\n",
    "    text = striptags(text)\n",
    "\n",
    "    # Tokenization\n",
    "    for term in text.split():  # default behavior of the split is to split on one or more whitespaces\n",
    "        # Lowercasing\n",
    "        term = term.lower()\n",
    "        # Stopword removal\n",
    "        if term in STOPWORDS:\n",
    "            continue\n",
    "        terms.append(term)\n",
    "\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the input document collection\n",
    "\n",
    "  - The collection is given as a single XML file. \n",
    "  - Each document is inside `<REUTERS ...> </REUTERS>`.\n",
    "  - We extract the contents of the `<DATE>`, `<TITLE>`, and `<BODY>` tags.\n",
    "  - After each extracted document, the provided callback function is called and all document data is passed in a single dict argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_collection(input_file, callback):\n",
    "    xmldoc = minidom.parse(input_file)\n",
    "    # Iterate documents in the XML file\n",
    "    itemlist = xmldoc.getElementsByTagName(\"REUTERS\")\n",
    "    doc_id = 0\n",
    "    for doc in itemlist:\n",
    "        doc_id += 1\n",
    "        date = doc.getElementsByTagName(\"DATE\")[0].firstChild.nodeValue\n",
    "        # Skip documents without a title or body\n",
    "        if not (doc.getElementsByTagName(\"TITLE\") and doc.getElementsByTagName(\"BODY\")):\n",
    "            continue\n",
    "        title = doc.getElementsByTagName(\"TITLE\")[0].firstChild.nodeValue\n",
    "        body = doc.getElementsByTagName(\"BODY\")[0].firstChild.nodeValue\n",
    "        callback({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"date\": date,\n",
    "            \"title\": title,\n",
    "            \"body\": body\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prints a document's contents (used as a callback function passed to `process_collection`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_doc(doc):\n",
    "    if doc[\"doc_id\"] <= 5:  # print only the first 5 documents\n",
    "        print(\"docID:\", doc[\"doc_id\"])\n",
    "        print(\"date:\", doc[\"date\"])\n",
    "        print(\"title:\", doc[\"title\"])\n",
    "        print(\"body:\", doc[\"body\"])\n",
    "        print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "process_collection(\"data/reuters21578-000.xml\", print_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index\n",
    "\n",
    "  - The inverted index is an object with methods for adding and fetching postings.\n",
    "  - The data is stored in a map, where keys are terms and values are lists of postings.\n",
    "  - Each posting is an object that holds the doc_id and an optional payload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Posting(object):\n",
    "    def __init__(self, doc_id, payload=None):\n",
    "        self.doc_id = doc_id\n",
    "        self.payload = payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InvertedIndex(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.index = {}\n",
    "    \n",
    "    def add_posting(self, term, doc_id, payload=None):\n",
    "        \"\"\"Adds a document to the posting list of a term.\"\"\"\n",
    "        if term not in self.index:  # if term not in index, initialize empty posting list\n",
    "            self.index[term] = []\n",
    "        # append new posting to the posting list\n",
    "        self.index[term].append(Posting(doc_id, payload))\n",
    "\n",
    "    def get_postings(self, term):\n",
    "        \"\"\"Fetches the posting list for a given term.\"\"\"\n",
    "        if term in self.index:\n",
    "            return self.index[term]\n",
    "        return None\n",
    "\n",
    "    def get_terms(self):\n",
    "        \"\"\"Returns all unique terms in the index.\"\"\"\n",
    "        return self.index.keys() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Create a unit test\n",
    "\n",
    "**TODO**: Complete the two tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestInvertedIndex(unittest.TestCase):\n",
    "\n",
    "    def test_postings(self):\n",
    "        ii = InvertedIndex()\n",
    "        # TODO: Test the posting list for two different terms (existent vs. non-existent)\n",
    "        self.assertEqual(0, 0)\n",
    "\n",
    "    def test_vocabulary(self):\n",
    "        ii = InvertedIndex()\n",
    "        # TODO: Test the vocabulary\n",
    "        self.assertEqual(0, 0)\n",
    "        \n",
    "        \n",
    "unittest.main(argv=['-q', 'TestInvertedIndex'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 2: Build an inverted index from the input collection\n",
    "\n",
    "**TODO**: Complete the code to index the entire document collection.  (The content for each document is the title and body concatenated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ind = InvertedIndex()\n",
    "\n",
    "def index_doc(doc):\n",
    "    text = doc[\"title\"] + \" \" + doc[\"body\"]\n",
    "    terms = parse(text)  # list of terms in the document\n",
    "    if doc[\"doc_id\"] <= 5:  # print preprocessed contents for the first 5 docs\n",
    "        print(\"docID:\", doc[\"doc_id\"])        \n",
    "        print(terms)\n",
    "\n",
    "    # TODO: index the document (add all terms with freqs using `ind.add_posting()`)\n",
    "    \n",
    "process_collection(\"data/reuters21578-000.xml\", index_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Save the inverted index to a file (`data/index.dat`). You may use a simple text file, e.g., with `termID docID1:freq1 docID2:freq2 ...` per line. \n",
    "\n",
    "Implement this by (1) adding a `write_to_file(self, filename)` method to the `InvertedIndex` class and then (2) invoking that method in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (advanced, optional)**: Create a plot that compares the size of the document collection (bytes) against the size of the corresponding index (bytes) on the y-axis vs. with respect to the number of documents on the x-axis. You may use [Matplotlib](https://www.tutorialspoint.com/jupyter/jupyter_notebook_plotting.htm) for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Use NLTK for text preprocessing\n",
    "\n",
    "**TODO**: Perprocess the text using the [tokenizer](http://www.nltk.org/howto/tokenize.html) and [Porter stemmer](http://www.nltk.org/howto/stem.html) in NLTK, but use the same stopwords list as before (i.e., defined in the `STOPWORDS` variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Build the index and save it to a file (`data/index_nltk.dat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Compare the first 10 words of the vocabulary using our simple text preprocessing methods (from Task 2) versus the vocabulary when using NLTK (Task 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task 4 (advanced, optional): Include bigrams in the index\n",
    "\n",
    "**TODO**: Create an index using bigrams, instead of unigrams. Hint: you may want to look at [some possible usages](https://www.programcreek.com/python/example/86315/nltk.bigrams) of [`nltk.util.bigrams()`](http://www.nltk.org/api/nltk.html#nltk.util.bigrams).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Build the index and save it to a file (`data/index_nltk_bigrams.dat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: How does the size of the vocabulary compare against using only unigrams (i.e., the index from Task 3)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/22o3ursi5YsR1Ztb8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
