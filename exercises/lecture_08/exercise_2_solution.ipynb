{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2: Retrieval evaluation, binary relevance\n",
    "\n",
    "  - Compute retrieval evaluation metrics using binary relevance: P@5, P@10, Average Precision, and Reciprocal Rank\n",
    "  - Compute the metrics for each query individually, as well as the averages over the entire query set\n",
    "  - Note: we use System A's ranking and the ground truth from the paper-based Exercise #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rankings produced for each query\n",
    "\n",
    "The key is the queryID, the value is a list of docIDs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rankings = {\n",
    "    \"q1\": [1, 2, 4, 5, 3, 6, 9, 8, 10, 7],\n",
    "    \"q2\": [1, 2, 4, 5, 3, 9, 8, 6, 10, 7],\n",
    "    \"q3\": [1, 7, 4, 5, 3, 6, 9, 8, 10, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth\n",
    "\n",
    "The key is the queryID, the value is a list of documents that are relevant for that query;  documents not listed here are irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gtruth = {\n",
    "    \"q1\": [1, 3],\n",
    "    \"q2\": [2, 4, 5, 6],\n",
    "    \"q3\": [7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query q1 \n",
      "\tranking: [1, 2, 4, 5, 3, 6, 9, 8, 10, 7] \n",
      "\tground truth: [1, 3]\n",
      "\tP@5: 0.4 \n",
      "\tP@10: 0.2 \n",
      "\tAP: 0.7 \n",
      "\tRR: 1.0\n",
      "Query q2 \n",
      "\tranking: [1, 2, 4, 5, 3, 9, 8, 6, 10, 7] \n",
      "\tground truth: [2, 4, 5, 6]\n",
      "\tP@5: 0.6 \n",
      "\tP@10: 0.4 \n",
      "\tAP: 0.604 \n",
      "\tRR: 0.5\n",
      "Query q3 \n",
      "\tranking: [1, 7, 4, 5, 3, 6, 9, 8, 10, 2] \n",
      "\tground truth: [7]\n",
      "\tP@5: 0.2 \n",
      "\tP@10: 0.1 \n",
      "\tAP: 0.5 \n",
      "\tRR: 0.5\n",
      "Average\n",
      "\tP@5: 0.4 \n",
      "\tP@10: 0.233 \n",
      "\tMAP: 0.601 \n",
      "\tMRR: 0.667\n"
     ]
    }
   ],
   "source": [
    "sum_p5, sum_p10, sum_ap, sum_rr = 0, 0, 0, 0\n",
    "\n",
    "for qid, ranking in sorted(rankings.items()):\n",
    "    gt = gtruth[qid]    \n",
    "    print(\"Query\", qid, \"\\n\\tranking:\", ranking, \"\\n\\tground truth:\", gt)\n",
    "    \n",
    "    p5, p10, ap, rr, num_rel = 0, 0, 0, 0, 0\n",
    "    \n",
    "    for i, doc_id in enumerate(ranking):\n",
    "        if doc_id in gt:  # doc is relevant\n",
    "            num_rel += 1  \n",
    "            pi = num_rel / (i + 1)  # P@i\n",
    "            ap += pi  # AP\n",
    "            \n",
    "            if i < 5:  # P@5\n",
    "                p5 += 1\n",
    "            if i < 10:  # P@10\n",
    "                p10 += 1\n",
    "            if rr == 0:  # Reciprocal rank\n",
    "                rr = 1 / (i + 1)\n",
    "    \n",
    "    p5 /= 5\n",
    "    p10 /= 10\n",
    "    ap /= len(gt)  # divide by the number of relevant documents\n",
    "    print(\"\\tP@5:\", round(p5, 3), \"\\n\\tP@10:\", round(p10, 3), \"\\n\\tAP:\", round(ap, 3), \"\\n\\tRR:\", round(rr, 3))\n",
    "    \n",
    "    \n",
    "    sum_p5 += p5\n",
    "    sum_p10 += p10\n",
    "    sum_ap += ap\n",
    "    sum_rr += rr\n",
    "\n",
    "print(\"Average\")\n",
    "print(\"\\tP@5:\", round(sum_p5 / len(rankings), 3), \"\\n\\tP@10:\", round(sum_p10 / len(rankings), 3), \n",
    "      \"\\n\\tMAP:\", round(sum_ap / len(rankings), 3), \"\\n\\tMRR:\", round(sum_rr / len(rankings), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback\n",
    "\n",
    "Please give (anonymous) feedback on this exercise by filling out [this form](https://forms.gle/22o3ursi5YsR1Ztb8)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
